<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>深度学习花书(6)-卷积神经网络 | Hexo</title>
  <meta name="keywords" content=" 卷积神经网络 ">
  <meta name="description" content="深度学习花书(6)-卷积神经网络 | Hexo">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="卷积神经网络本节介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 https:&#x2F;&#x2F;zh.d2l.ai&#x2F;chapter_convolutional-neural-networks&#x2F;index.html 5.1、">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习花书(6)-卷积神经网络">
<meta property="og:url" content="http://example.com/2022/06/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-6-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="卷积神经网络本节介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 https:&#x2F;&#x2F;zh.d2l.ai&#x2F;chapter_convolutional-neural-networks&#x2F;index.html 5.1、">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138635.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138290.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138564.png">
<meta property="article:published_time" content="2022-06-19T13:36:44.000Z">
<meta property="article:modified_time" content="2022-06-19T13:43:43.670Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138635.png">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 6.2.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/avatar.jpg"/>
</a>
<div class="author">
    <span>John Doe</span>
</div>

<div class="icon">
    
        
            <a title="rss"
               href="/atom.xml"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-rss"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="github"
               href="https://github.com/LIyvqi/LIyvqi.github.io"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-github"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="zhihu"
               href="https://www.zhihu.com/people/li-yu-qi-11-23"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-zhihu"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="qq"
               href="http://wpa.qq.com/msgrd?v=3&uin=1279562957&site=qq&menu=yes"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-qq"></use>
                    </svg>
                
            </a>
        
    
</div>



    <a class="more-menus">更多菜单</a>


<ul>
    <li>
        <div class="all active" data-rel="All">All
            
                <small>(10)</small>
            
        </div>
    </li>
    
        
            
                <li>
                    <div data-rel="Love">
                        
                        Love
                        <small>(1)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="深度学习">
                        
                        深度学习
                        <small>(6)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="运维">
                        
                        运维
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
                <a class="dynamic-menu site_url"
                   
                   href="/photo">相册</a>
        
    </div>
    <div>
        
            <a class="about  site_url"
               
               href="/about">About</a>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="10">
<input type="hidden" id="yelog_site_word_count" value="11.7k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        Links
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">All</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>参数控制</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>读写模型</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>多层感知机</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>房价预测</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>过（欠）拟合</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>建站</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>教程</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>卷积神经网络</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>块</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>老婆生日</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>逻辑回归</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>深度学习</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>梯度消失和爆炸</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>微积分</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>问题</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>写作</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>预备知识</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>自定义层</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>GPU调用</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>kaggle实例</a>
            </li>
        
    </div>

</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a  class="All 深度学习 "
           href="/2022/06/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-6-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
           data-tag="卷积神经网络"
           data-author="" >
            <span class="post-title" title="深度学习花书(6)-卷积神经网络">深度学习花书(6)-卷积神经网络</span>
            <span class="post-date" title="2022-06-19 21:36:44">2022/06/19</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-5-%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97/"
           data-tag="块,参数控制,自定义层,读写模型,GPU调用"
           data-author="" >
            <span class="post-title" title="深度学习花书(5)-深度模型计算">深度学习花书(5)-深度模型计算</span>
            <span class="post-date" title="2022-06-17 16:12:55">2022/06/17</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6%EF%BC%884%EF%BC%89-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8B/"
           data-tag="多层感知机,房价预测,kaggle实例"
           data-author="" >
            <span class="post-title" title="深度学习花书（4）-多层感知机下">深度学习花书（4）-多层感知机下</span>
            <span class="post-date" title="2022-06-16 21:40:50">2022/06/16</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8A/"
           data-tag="多层感知机,过（欠）拟合,梯度消失和爆炸"
           data-author="" >
            <span class="post-title" title="深度学习花书(3)-多层感知机上">深度学习花书(3)-多层感知机上</span>
            <span class="post-date" title="2022-06-15 22:46:18">2022/06/15</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"
           data-tag="深度学习,逻辑回归"
           data-author="" >
            <span class="post-title" title="深度学习花书(2)-逻辑回归">深度学习花书(2)-逻辑回归</span>
            <span class="post-date" title="2022-06-14 18:47:00">2022/06/14</span>
        </a>
        
        <a  class="All Love "
           href="/2022/06/13/2022%E5%B9%B4%E8%80%81%E5%A9%86%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/"
           data-tag="老婆生日"
           data-author="" >
            <span class="post-title" title="2022年老婆生日快乐">2022年老婆生日快乐</span>
            <span class="post-date" title="2022-06-13 21:38:00">2022/06/13</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6Day1-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"
           data-tag="深度学习,预备知识,微积分"
           data-author="" >
            <span class="post-title" title="深度学习花书-预备知识">深度学习花书-预备知识</span>
            <span class="post-date" title="2022-06-13 21:15:13">2022/06/13</span>
        </a>
        
        <a  class="All 运维 "
           href="/2022/06/12/Hexo%E5%86%99%E5%8D%9A%E5%AE%A2%E4%B8%AD%E4%BC%9A%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"
           data-tag="写作,问题"
           data-author="" >
            <span class="post-title" title="Hexo写博客中会遇到的问题（持续更新）">Hexo写博客中会遇到的问题（持续更新）</span>
            <span class="post-date" title="2022-06-12 19:32:54">2022/06/12</span>
        </a>
        
        <a  class="All 运维 "
           href="/2022/06/12/Hexo-GitHub%E9%85%8D%E7%BD%AE%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"
           data-tag="教程,建站"
           data-author="" >
            <span class="post-title" title="Hexo-GitHub配置个人网站">Hexo-GitHub配置个人网站</span>
            <span class="post-date" title="2022-06-12 18:22:32">2022/06/12</span>
        </a>
        
        <a  class="All "
           href="/2022/06/12/hello-world/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hello World(作者自带的，感谢作者，就不删啦)">Hello World(作者自带的，感谢作者，就不删啦)</span>
            <span class="post-date" title="2022-06-12 11:12:53">2022/06/12</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-深度学习花书-6-卷积神经网络" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">深度学习花书(6)-卷积神经网络</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a  data-rel="深度学习">深度学习</a>
            
        </span>
        
        
        <span class="tag">
            <i class="iconfont icon-tag"></i>
            
            <a class="color2">卷积神经网络</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
            Created At : <time class="date" title='Updated At: 2022-06-19 21:43:43'>2022-06-19 21:36</time>
        
    </div>
    <div class="article-meta">
        
        <span>Count:3.2k</span>
        
        
        <span id="busuanzi_container_page_pv">
            Views 👀 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1%E3%80%81%E5%86%99%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%EF%BC%8C%E4%B8%8D%E7%86%9F%E6%82%89%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-text">5.1、写在最前面，不熟悉的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2%E3%80%81%E5%8D%B7%E7%A7%AF%E4%BB%8B%E7%BB%8D"><span class="toc-text">5.2、卷积介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3%E3%80%81%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-text">5.3、多输入输出通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4%E3%80%81Pooling"><span class="toc-text">5.4、Pooling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5%E3%80%81%E5%AE%9E%E6%88%98%E4%BE%8B%E5%AD%90%E2%80%94%E2%80%94LeNet"><span class="toc-text">5.5、实战例子——LeNet</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>本节介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。</p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html</a></p>
<h3 id="5-1、写在最前面，不熟悉的代码"><a href="#5-1、写在最前面，不熟悉的代码" class="headerlink" title="5.1、写在最前面，不熟悉的代码"></a>5.1、写在最前面，不熟悉的代码</h3><p>要学会调用卷积层和池化层，也要自己会写卷积层和池化层。</p>
<pre><code class="python">nn.Conv2d()
in_channels：输入的通道数目 【必选】
out_channels： 输出的通道数目 【必选】
kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】
stride： 卷积每次滑动的步长为多少，默认是 1 【可选】
padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】
参数的详解出自：https://blog.csdn.net/qq_38863413/article/details/104108808

conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape

conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
# 不光填充可以高度和宽度不一样，步幅也可以高度和宽度不一样
# 元组里面先高度后宽度

torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)

kernel_size(int or tuple) ：max pooling的窗口大小
stride(int or tuple, optional)：max pooling的窗口移动的步长。默认值是kernel_size
padding(int or tuple, optional) ：输入的每一条边补充0的层数
dilation(int or tuple, optional)：一个控制窗口中元素步幅的参数
return_indices ：如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助
ceil_mode ：如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的

# 打印每一层的形状
for layer in net:
    X = layer(X)  # 调用网络中的每一层，计算
    print(layer.__class__.__name__,&#39;output shape: \t&#39;,X.shape)
    
# 自己实现一个卷积层(有些面试常问的)
def corr2d(X, K):  #@save
    &quot;&quot;&quot;计算二维互相关运算，了解即可，后面直接掉包&quot;&quot;&quot;
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
# 卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 
# 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
# 高度和宽度分别为ℎh和𝑤w的卷积核可以被称为ℎ×𝑤h×w卷积或ℎ×𝑤h×w卷积核。 
# 我们也将带有ℎ×𝑤h×w卷积核的卷积层称为ℎ×𝑤h×w卷积层。
</code></pre>
<h3 id="5-2、卷积介绍"><a href="#5-2、卷积介绍" class="headerlink" title="5.2、卷积介绍"></a>5.2、卷积介绍</h3><p>卷积神经网络正是将<em>空间不变性</em>（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。设计适合于计算机视觉的神经网络架构，应具备以下两点：</p>
<ol>
<li><p><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</p>
</li>
<li><p><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p>
</li>
</ol>
<p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<em>互相关运算</em>（cross-correlation），而不是卷积运算。 在卷积层中，输入张量和核张量通过(<strong>互相关运算</strong>)产生输出张量。</p>
<p>一个卷积核的输出计算：</p>
<p><img src="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138635.png" alt="img"></p>
<pre><code class="Python">def corr2d(X, K):  #@save
    &quot;&quot;&quot;计算二维互相关运算，了解即可，后面直接掉包&quot;&quot;&quot;
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
# 卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 
# 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
# 高度和宽度分别为ℎh和𝑤w的卷积核可以被称为ℎ×𝑤h×w卷积或ℎ×𝑤h×w卷积核。 
# 我们也将带有ℎ×𝑤h×w卷积核的卷积层称为ℎ×𝑤h×w卷积层。
</code></pre>
<p>卷积的填充，因为卷积核是一个小卷积，在卷积的过程中会出现输入的规模减小，我们可以对边缘进行填充来减少这个现象。</p>
<pre><code class="Python"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) 
X = torch.rand(size=(8, 8))
X = X.reshape((1, 1) + X.shape)  # 这里的（1，1）表示批量大小和通道数都是1
Y = conv2d(X)
Y.shape  # torch.Size([1, 1, 8, 8])
</code></pre>
<p>nn.Conv2d()</p>
<pre><code class="Plain">in_channels：输入的通道数目 【必选】
out_channels： 输出的通道数目 【必选】
kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。【必选】
stride： 卷积每次滑动的步长为多少，默认是 1 【可选】
padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。【可选】
参数的详解出自：https://blog.csdn.net/qq_38863413/article/details/104108808
</code></pre>
<p>当卷积核的高度和宽度不同时，我们可以[<strong>填充不同的高度和宽度</strong>]，使输出和输入具有相同的高度和宽度。在如下示例中，<strong>我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。</strong></p>
<pre><code class="Python">conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape

conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
# 不光填充可以高度和宽度不一样，步幅也可以高度和宽度不一样
# 元组里面先高度后宽度
</code></pre>
<p>默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充</p>
<h3 id="5-3、多输入输出通道"><a href="#5-3、多输入输出通道" class="headerlink" title="5.3、多输入输出通道"></a>5.3、多输入输出通道</h3><p>当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有3×ℎ×𝑤的形状。我们将这个大小为3的轴称为<em>通道</em>（channel）维度。</p>
<p><img src="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138290.png" alt="img"> </p>
<p>多通道计算的实现</p>
<pre><code class="Python">X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])

def corr2d_multi_in(X, K):
    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))

def corr2d_multi_in_out(X, K):
    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。
    # 最后将所有结果都叠加在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)

corr2d_multi_in_out(X, K)
</code></pre>
<h3 id="5-4、Pooling"><a href="#5-4、Pooling" class="headerlink" title="5.4、Pooling"></a>5.4、Pooling</h3><p>双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p>
<p>不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。</p>
<pre><code class="Python"># 了解即可，一个汇聚层的代码
def pool2d(X, pool_size, mode=&#39;max&#39;):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))   # 设置输出的窗口的大小
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == &#39;max&#39;:
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == &#39;avg&#39;:
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
</code></pre>
<p>torch.nn.MaxPool2d(kernel_size, stride&#x3D;None, padding&#x3D;0, dilation&#x3D;1, return_indices&#x3D;False, ceil_mode&#x3D;False)</p>
<pre><code class="Plain">kernel_size(int or tuple) ：max pooling的窗口大小
stride(int or tuple, optional)：max pooling的窗口移动的步长。默认值是kernel_size
padding(int or tuple, optional) ：输入的每一条边补充0的层数
dilation(int or tuple, optional)：一个控制窗口中元素步幅的参数
return_indices ：如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助
ceil_mode ：如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的
</code></pre>
<p>在处理多通道输入数据时，[<strong>汇聚层在每个输入通道上单独运算</strong>]，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。</p>
<h3 id="5-5、实战例子——LeNet"><a href="#5-5、实战例子——LeNet" class="headerlink" title="5.5、实战例子——LeNet"></a>5.5、实战例子——LeNet</h3><p><img src="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138564.png" alt="img"></p>
<p>LeNet的组成部分有两个卷积层和三个全连接层实现，我们在这里直接学习一下如何调用现成的。</p>
<p>通过下面的LeNet代码，你会相信用深度学习框架实现此类模型非常简单。我们只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。</p>
<pre><code class="Python">import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))

# 为了检查模型，打印每一层的形状
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)  # 调用网络中的每一层，计算
    print(layer.__class__.__name__,&#39;output shape: \t&#39;,X.shape)

# 模型的训练
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    &quot;&quot;&quot;使用GPU计算模型在数据集上的精度，这一部分其实从学习方法来讲不重要，无所谓&quot;&quot;&quot;
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

# 模型的训练部分，也算是重点
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    &quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;
    # 首先是初始化网络的参数
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    
    print(&#39;training on&#39;, device)
    net.to(device)   # 把网络放入设备中
    
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    # 指定优化器和损失函数
    
    animator = d2l.Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs],
                            legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])
    timer, num_batches = d2l.Timer(), len(train_iter)
    
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f&#39;loss &#123;train_l:.3f&#125;, train acc &#123;train_acc:.3f&#125;, &#39;
          f&#39;test acc &#123;test_acc:.3f&#125;&#39;)
    print(f&#39;&#123;metric[2] * num_epochs / timer.sum():.1f&#125; examples/sec &#39;
          f&#39;on &#123;str(device)&#125;&#39;)
  
# 了解训练的基本过程就行，因为这个训练的代码集成了很多作者自己写的d2l的库，实际中不多
# 主要是学习模型部分，代码的训练过程几乎开源的项目都有，找到自己适合的就行
# 但是，一定要记住的是，在模型的训练中有很多坑，这个要整理好
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</code></pre>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1279562957@qq.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">💰</a>
</p>






    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2022-2022 李爱可
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>Help us with donation</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">alipay</label></span><span><label><input type="radio" name="pay" value="weixin">weixin</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>






<div class="mobile-menus-out" >

</div>
<div class="mobile-menus">
    
    
    <a class="dynamic-menu site_url"   href="/photo">相册</a>
    
</div>


</html>
