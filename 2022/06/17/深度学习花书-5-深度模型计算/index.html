<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>深度学习花书(5)-深度模型计算 | Hexo</title>
  <meta name="keywords" content=" 块 , 参数控制 , 自定义层 , 读写模型 , GPU调用 ">
  <meta name="description" content="深度学习花书(5)-深度模型计算 | Hexo">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="卷积神经网络本节介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 https:&#x2F;&#x2F;zh.d2l.ai&#x2F;chapter_convolutional-neural-networks&#x2F;index.html 5.1、">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习花书(6)-卷积神经网络">
<meta property="og:url" content="http://example.com/2022/06/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-6-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="卷积神经网络本节介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 https:&#x2F;&#x2F;zh.d2l.ai&#x2F;chapter_convolutional-neural-networks&#x2F;index.html 5.1、">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138635.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138290.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138564.png">
<meta property="article:published_time" content="2022-06-19T13:36:44.000Z">
<meta property="article:modified_time" content="2022-06-19T13:43:43.670Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LIyvqi/FigOfWeb/main/202206192138635.png">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 6.2.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/avatar.jpg"/>
</a>
<div class="author">
    <span>John Doe</span>
</div>

<div class="icon">
    
        
            <a title="rss"
               href="/atom.xml"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-rss"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="github"
               href="https://github.com/LIyvqi/LIyvqi.github.io"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-github"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="zhihu"
               href="https://www.zhihu.com/people/li-yu-qi-11-23"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-zhihu"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="qq"
               href="http://wpa.qq.com/msgrd?v=3&uin=1279562957&site=qq&menu=yes"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-qq"></use>
                    </svg>
                
            </a>
        
    
</div>



    <a class="more-menus">更多菜单</a>


<ul>
    <li>
        <div class="all active" data-rel="All">All
            
                <small>(10)</small>
            
        </div>
    </li>
    
        
            
                <li>
                    <div data-rel="Love">
                        
                        Love
                        <small>(1)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="深度学习">
                        
                        深度学习
                        <small>(6)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="运维">
                        
                        运维
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
                <a class="dynamic-menu site_url"
                   
                   href="/photo">相册</a>
        
    </div>
    <div>
        
            <a class="about  site_url"
               
               href="/about">About</a>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="10">
<input type="hidden" id="yelog_site_word_count" value="11.7k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        Links
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">All</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>参数控制</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>读写模型</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>多层感知机</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>房价预测</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>过（欠）拟合</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>建站</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>教程</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>卷积神经网络</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>块</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>老婆生日</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>逻辑回归</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>深度学习</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>梯度消失和爆炸</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>微积分</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>问题</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>写作</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>预备知识</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>自定义层</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>GPU调用</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>kaggle实例</a>
            </li>
        
    </div>

</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a  class="All 深度学习 "
           href="/2022/06/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-6-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
           data-tag="卷积神经网络"
           data-author="" >
            <span class="post-title" title="深度学习花书(6)-卷积神经网络">深度学习花书(6)-卷积神经网络</span>
            <span class="post-date" title="2022-06-19 21:36:44">2022/06/19</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-5-%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97/"
           data-tag="块,参数控制,自定义层,读写模型,GPU调用"
           data-author="" >
            <span class="post-title" title="深度学习花书(5)-深度模型计算">深度学习花书(5)-深度模型计算</span>
            <span class="post-date" title="2022-06-17 16:12:55">2022/06/17</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6%EF%BC%884%EF%BC%89-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8B/"
           data-tag="多层感知机,房价预测,kaggle实例"
           data-author="" >
            <span class="post-title" title="深度学习花书（4）-多层感知机下">深度学习花书（4）-多层感知机下</span>
            <span class="post-date" title="2022-06-16 21:40:50">2022/06/16</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8A/"
           data-tag="多层感知机,过（欠）拟合,梯度消失和爆炸"
           data-author="" >
            <span class="post-title" title="深度学习花书(3)-多层感知机上">深度学习花书(3)-多层感知机上</span>
            <span class="post-date" title="2022-06-15 22:46:18">2022/06/15</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"
           data-tag="深度学习,逻辑回归"
           data-author="" >
            <span class="post-title" title="深度学习花书(2)-逻辑回归">深度学习花书(2)-逻辑回归</span>
            <span class="post-date" title="2022-06-14 18:47:00">2022/06/14</span>
        </a>
        
        <a  class="All Love "
           href="/2022/06/13/2022%E5%B9%B4%E8%80%81%E5%A9%86%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/"
           data-tag="老婆生日"
           data-author="" >
            <span class="post-title" title="2022年老婆生日快乐">2022年老婆生日快乐</span>
            <span class="post-date" title="2022-06-13 21:38:00">2022/06/13</span>
        </a>
        
        <a  class="All 深度学习 "
           href="/2022/06/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6Day1-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"
           data-tag="深度学习,预备知识,微积分"
           data-author="" >
            <span class="post-title" title="深度学习花书-预备知识">深度学习花书-预备知识</span>
            <span class="post-date" title="2022-06-13 21:15:13">2022/06/13</span>
        </a>
        
        <a  class="All 运维 "
           href="/2022/06/12/Hexo%E5%86%99%E5%8D%9A%E5%AE%A2%E4%B8%AD%E4%BC%9A%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"
           data-tag="写作,问题"
           data-author="" >
            <span class="post-title" title="Hexo写博客中会遇到的问题（持续更新）">Hexo写博客中会遇到的问题（持续更新）</span>
            <span class="post-date" title="2022-06-12 19:32:54">2022/06/12</span>
        </a>
        
        <a  class="All 运维 "
           href="/2022/06/12/Hexo-GitHub%E9%85%8D%E7%BD%AE%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"
           data-tag="教程,建站"
           data-author="" >
            <span class="post-title" title="Hexo-GitHub配置个人网站">Hexo-GitHub配置个人网站</span>
            <span class="post-date" title="2022-06-12 18:22:32">2022/06/12</span>
        </a>
        
        <a  class="All "
           href="/2022/06/12/hello-world/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hello World(作者自带的，感谢作者，就不删啦)">Hello World(作者自带的，感谢作者，就不删啦)</span>
            <span class="post-date" title="2022-06-12 11:12:53">2022/06/12</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-深度学习花书-5-深度模型计算" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">深度学习花书(5)-深度模型计算</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a  data-rel="深度学习">深度学习</a>
            
        </span>
        
        
        <span class="tag">
            <i class="iconfont icon-tag"></i>
            
            <a class="color2">块</a>
            
            <a class="color5">参数控制</a>
            
            <a class="color5">自定义层</a>
            
            <a class="color5">读写模型</a>
            
            <a class="color1">GPU调用</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
            Created At : <time class="date" title='Updated At: 2022-06-17 16:16:04'>2022-06-17 16:12</time>
        
    </div>
    <div class="article-meta">
        
        <span>Count:3.3k</span>
        
        
        <span id="busuanzi_container_page_pv">
            Views 👀 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E3%80%81%E5%86%99%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%EF%BC%8C%E4%B8%8D%E7%86%9F%E6%82%89%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-text">4.1、写在最前面，不熟悉的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E3%80%81%E5%9D%97"><span class="toc-text">4.2、块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E3%80%81%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%8F%82%E6%95%B0%E7%9A%84%E7%AE%A1%E7%90%86%EF%BC%9A%E8%AE%BF%E9%97%AE%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%81%E5%85%B1%E4%BA%AB"><span class="toc-text">4.3、模型中参数的管理：访问、初始化、共享</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-text">4.4、自定义层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E3%80%81%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-text">4.5、读写文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E3%80%81GPU%E7%9A%84%E8%B0%83%E7%94%A8"><span class="toc-text">4.6、GPU的调用</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_deep-learning-computation/index.html">https://zh.d2l.ai/chapter_deep-learning-computation/index.html</a>  </p>
<p>参考资料见上</p>
<h3 id="4-1、写在最前面，不熟悉的代码"><a href="#4-1、写在最前面，不熟悉的代码" class="headerlink" title="4.1、写在最前面，不熟悉的代码"></a>4.1、写在最前面，不熟悉的代码</h3><pre><code class="Python"># 一个自定义块的写法，等同于下面直接掉包
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层
        
    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))

# 当然也可以直接掉包
from troch import nn
new_MLP = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))

net[][].state_dict() # 访问某一层网络的参数，按照数组的格式访问
print(net[2].bias)   # tensor([0.1957], requires_grad=True)
print(net[2].bias.data) # tensor([0.1957])
net[2].weight.grad    # 访问梯度，在反向传播之前是负的梯度
print(*[(name, param.shape) for name, param in net.named_parameters()]) 
# 一次性访问所有参数
net.state_dict()[&#39;2.bias&#39;].data  按照字典访问参数

# 参数的初始化方法
def init_normal(m):
    if type(m) == nn.Linear:  # 把所有的nn.Linear类型的层按照指定的分布初始化
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # nn.init.constant_(m.weight, 1) 初始化为常数1
        nn.init.zeros_(m.bias)
net.apply(init_normal)   # 也可以net[0].apply(init_normal)，初始化某一层

&quot;&quot;&quot;
nn.Module里面关于参数有两个很重要的属性named_parameters()和parameters()，
前者给出网络层的名字和参数的迭代器，而后者仅仅是参数的迭代器。
&quot;&quot;&quot;

# 层组成块，我们除了可以自定义块，还可以自定义层
# 实现一个全连接层
class MyLinear(nn.Module):
    def __init__(self, inputs, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bais.data
        return F.relu(linear)

# 读写文件，保存参数和模型
torch.save(x, &#39;x-file&#39;)
x2 = torch.load(&#39;x-file&#39;)
torch.save(net.state_dict(), &#39;mlp.params&#39;)
# 为了恢复模型，我们[实例化了原始多层感知机模型的一个备份。] 
# 这里我们不需要随机初始化模型参数，而是(直接读取文件中存储的参数。)
clone = MLP()
clone.load_state_dict(torch.load(&#39;mlp.params&#39;))   # 模型加载参数
clone.eval()
# 比较灵活，直接存就行，存的是什么格式，读出来就是什么格式

module.train()   # 启用 Batch Normalization 和 Dropout。如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。
module.eval()    # 不启用 Batch Normalization 和 Dropout。如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()

device = torch.device(f&#39;cuda:&#123;i&#125;&#39;) 
X = torch.ones(2, 3, device=try_gpu())
# 指定在哪一个GPU上创建
# 将网络的模型指定在哪一个gpu上
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
</code></pre>
<h3 id="4-2、块"><a href="#4-2、块" class="headerlink" title="4.2、块"></a>4.2、块</h3><p><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的。</p>
<p><strong>深度神经网络的模型就是一个块一个块像搭积木一样搭起来的，很重要。</strong></p>
<p>如何自定义一个块，每个块必须提供以下基本功能：</p>
<ol>
<li><p>将输入数据作为其前向传播函数的参数。</p>
</li>
<li><p>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。</p>
</li>
<li><p>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</p>
</li>
<li><p>存储和访问前向传播计算所需的参数。</p>
</li>
<li><p>根据需要初始化模型参数。</p>
</li>
</ol>
<pre><code class="Python"># 一个块的标准定义
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))

# 当然也可以直接掉包
from troch import nn
new_MLP = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
</code></pre>
<p>上述的nn.Sequential()是一个顺序块，就是提前定义好的块，我们可以自己定义一下：</p>
<pre><code class="Python">class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在&#39;Module&#39;类的成员
            # 变量_modules中。module的类型是OrderedDict
            self._modules[str(idx)] = module
    
    def forward(self,X):
        # OrderedDict 保证了成员变量添加的属性遍历他们
        for block in self._modules.values():
            X = block(X)
        return X
</code></pre>
<p>对于一个块来说，是你可以随心所欲设计的，想怎么组合怎么混合都可以。</p>
<h3 id="4-3、模型中参数的管理：访问、初始化、共享"><a href="#4-3、模型中参数的管理：访问、初始化、共享" class="headerlink" title="4.3、模型中参数的管理：访问、初始化、共享"></a>4.3、模型中参数的管理：访问、初始化、共享</h3><p>从已有模型中访问参数。 当通过<code>Sequential</code>类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。 如下所示，我们可以检查第二个全连接层的参数。 注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。</p>
<pre><code class="Python">net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
print(net[2].state_dict())  # 这就是访问的 nn.Linear(8, 1)的参数
# OrderedDict([(&#39;weight&#39;, tensor([[-0.1976,  0.2988,  0.0623, -0.2936, -0.2918, -0.2031,  0.1552,  0.0943]])), 
# (&#39;bias&#39;, tensor([0.1957]))])
</code></pre>
<p>访问目标参数，比如访问第三个神经网络层（nn.Linear(8,1)）的偏置。</p>
<pre><code class="Python">print(net[2].bias)   # tensor([0.1957], requires_grad=True)
print(net[2].bias.data) # tensor([0.1957])
net[2].weight.grad    # 访问梯度，在反向传播之前是负的梯度
print(*[(name, param.shape) for name, param in net.named_parameters()]) # 一次性访问所有参数
net.state_dict()[&#39;2.bias&#39;].data  按照字典访问参数
</code></pre>
<p>深度神经网络的网络层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。</p>
<p>可以直接print(net)查看，索引是从0开始，可以按照net[][]的格式访问。</p>
<p><strong>参数的初始化：</strong></p>
<pre><code class="Python"># 方法一
def init_normal(m):
    if type(m) == nn.Linear:  # 把所有的nn.Linear类型的层按照指定的分布初始化
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # nn.init.constant_(m.weight, 1) 初始化为常数1
        nn.init.zeros_(m.bias)
net.apply(init_normal)

# 方法二
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier)   # 指定这一个层使用xavier初始化方法，如果没有[0]指定的话是针对所有的层
net[2].apply(init_42)

# 方法三 自定义的参数更新，其实无论怎么样，我们只要自定义一个参数更新的方式
# 然后 nn.apply(my_init)
def my_init(m):
    if type(m) == nn.Linear:
        # print(&quot;Init&quot;, *[(name, param.shape)
        #                 for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() &gt;= 5

net.apply(my_init)
</code></pre>
<p>nn.Module里面关于参数有两个很重要的属性named_parameters()和parameters()，前者给出<strong>网络层的名字和参数</strong>的迭代器，而后者<strong>仅仅是参数</strong>的迭代器。</p>
<p>参数绑定（参数共享）</p>
<p>有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p>
<pre><code class="Python"># 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared,
                    nn.ReLU(),
                    shared,
                    nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
</code></pre>
<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 你可能会思考：当参数绑定时，梯度会发生什么情况？ <strong>答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起</strong>。</p>
<h3 id="4-4、自定义层"><a href="#4-4、自定义层" class="headerlink" title="4.4、自定义层"></a>4.4、自定义层</h3><p>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。</p>
<p> 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。</p>
<pre><code class="Python"># 实现一个全连接层
class MyLinear(nn.Module):
    def __init__(self, inputs, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bais.data
        return F.relu(linear)
        
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
</code></pre>
<h3 id="4-5、读写文件"><a href="#4-5、读写文件" class="headerlink" title="4.5、读写文件"></a>4.5、读写文件</h3><p>加载和保存张量   torch.save()   torch.load()</p>
<pre><code class="Python">x = torch.arange(4)
torch.save(x, &#39;x-file&#39;)
x2 = torch.load(&#39;x-file&#39;)
y = torch.zeros(4)
torch.save([x, y],&#39;x-files&#39;)
mydict = &#123;&#39;x&#39;: x, &#39;y&#39;: y&#125;
torch.save(mydict, &#39;mydict&#39;)
# 比较灵活，直接存就行，存的是什么格式，读出来就是什么格式
</code></pre>
<p>保存整个模型，其实保存的就是这个模型的参数。</p>
<pre><code class="Python">torch.save(net.state_dict(), &#39;mlp.params&#39;)
# 为了恢复模型，我们[实例化了原始多层感知机模型的一个备份。] 
# 这里我们不需要随机初始化模型参数，而是(直接读取文件中存储的参数。)
clone = MLP()
clone.load_state_dict(torch.load(&#39;mlp.params&#39;))   # 模型加载参数
clone.eval()
</code></pre>
<p>**model.train()**的作用是启用 Batch Normalization <strong>和</strong> Dropout。如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p>
<p>**model.eval()**的作用是不启用 Batch Normalization <strong>和</strong> Dropout。如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。</p>
<h3 id="4-6、GPU的调用"><a href="#4-6、GPU的调用" class="headerlink" title="4.6、GPU的调用"></a>4.6、GPU的调用</h3><pre><code class="Python">def try_gpu(i=0):  #@save
    &quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;
    if torch.cuda.device_count() &gt;= i + 1:
        return torch.device(f&#39;cuda:&#123;i&#125;&#39;)
    return torch.device(&#39;cpu&#39;)

def try_all_gpus():  #@save
    &quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;
    devices = [torch.device(f&#39;cuda:&#123;i&#125;&#39;)
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device(&#39;cpu&#39;)]
</code></pre>
<p>默认情况下，张量是在CPU上创建的，无论何时我们要对多个项进行操作， <strong>它们都必须在同一个设备上，且区分GPU1和GPU2。</strong> 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</p>
<pre><code class="Python">X = torch.ones(2, 3, device=try_gpu())
# 指定在哪一个GPU上创建
# 将网络的模型指定在哪一个gpu上
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
</code></pre>
<p>不经意地移动数据可能会显著降低性能。<strong>一个典型的错误如下</strong>：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy <code>ndarray</code>中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。</p>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1279562957@qq.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">💰</a>
</p>






    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2022-2022 李爱可
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>Help us with donation</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">alipay</label></span><span><label><input type="radio" name="pay" value="weixin">weixin</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>






<div class="mobile-menus-out" >

</div>
<div class="mobile-menus">
    
    
    <a class="dynamic-menu site_url"   href="/photo">相册</a>
    
</div>


</html>
